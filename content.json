{"meta":{"title":"Kuro's Blog","subtitle":"Kuro","description":"坚持 是一种品格","author":"Kuro","url":"https://midkuro.github.io","root":"/"},"pages":[{"title":"about","date":"2020-05-20T09:29:27.000Z","updated":"2020-05-20T09:30:17.111Z","comments":true,"path":"about/index.html","permalink":"https://midkuro.github.io/about/index.html","excerpt":"","text":"关于我从事 JAVA 后台开发，主要开发语言 PHP，熟悉使用 Spring Boot、Spring Cloud 等主流框架；熟悉JVM内存运行区域、类加载机制，熟悉多线程编程，线程安全设计。 对DevOps有一定的了解。编写代码遵循SonarLint检测。 热爱开源项目、热爱新技术、热爱新事物。 关于工作城市：深圳南山区 关于学习正在往终身学习者前进…近期学习方向：NIO 关于座右铭 坚持 是一种品格 关于爱好热爱运动，喜爱羽毛球、看小说。 联系我 Blog: midkuro.io GitHub: midkuro Email: 276302007@qq.com"}],"posts":[{"title":"'kubernetes 环境搭建'","slug":"kubernetes-install","date":"2020-05-21T14:00:00.000Z","updated":"2020-05-21T12:30:28.408Z","comments":true,"path":"2020/05/21/kubernetes-install/","link":"","permalink":"https://midkuro.github.io/2020/05/21/kubernetes-install/","excerpt":"","text":"K8S的安装方式最简单的方法是使用yum install kubernetes命令安装Kubernetes集群，但仍需修改各组件的启动参数，才能完成对Kubernetes集群的配置，整个过程比较复杂，也容易出错。但是对于新手来说是一个熟悉k8s的一个过程，可以适当借鉴学习。 Kubernetes从1.4版本开始引入了命令行工具kubeadm，致力于简化集群的安装过程，并解决Kubernetes集群的高可用问题。在Kubernetes 1.13版本中，kubeadm工具进入GA阶段，宣称已经为生产环境应用准备就绪。比较推荐使用这种方式，安装便捷并且容错率高 本节先讲解基于 yum install kubernetes命令安装 Master节点：192.168.1.132 Node节点：192.168.1.134 K8S基于yum的安装安装前先关闭防火墙 1systemctl stop firewalld 修改系统文件/etc/sysconfig/selinux，将SELINUX=enforcing修改成SELINUX=disabled，然后重启Linux。或者执行以下命令 1setenforce 0 编辑/etc/sysctl.conf添加以下内容 123456net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward=1vm.swappiness = 0fs.inotify.maxuserwatches = 24576vm.max_map_count=655360 然后执行命令 sysctl -p,如果报错，执行命令modprobe br_netfilter 安装Master节点1、安装Docker 安装docker教程 2、Master节点安装etcd 1yum install etcd -y etcd用于K8S的数据存储，原生支持做集群，修改/etc/etcd/etcd.conf配置,指向Master节点 123[root@localhost /]# vim /etc/etcd/etcd.conf6 行：ETCD_LISTEN_CLIENT_URLS=\"http://0.0.0.0:2379\"22行：ETCD_ADVERTISE_CLIENT_URLS=\"http://192.168.1.132:2379\" 启动etcd服务并且设置开机自启 12[root@localhost /]# systemctl start etcd.service[root@localhost /]# systemctl enable etcd.service 检查 12345678[root@localhost /]# netstat -lntup|grep 2379tcp6 0 0 :::2379 :::* LISTEN 117472/etcd#检查健康状态[root@localhost /]# etcdctl -C http://192.168.1.132:2379 cluster-healthmember 8e9e05c52164694d is healthy: got healthy result from http://192.168.1.132:2379cluster is healthy 测试 1234567891011# 插入数据 键（目录）值（123456）[root@localhost /]# etcdctl set /test/word 123456 123456[root@localhost /]# etcdctl ls //test[root@localhost /]# etcdctl ls /test/test/word[root@localhost /]# etcdctl get /test/word # 查看值123456[root@localhost /]#etcdctl rm /test/word # 删除键值对[root@localhost /]#etcdctl rmdir /test # 删除目录 3、Master节点192.168.1.132安装K8S 以下命令根据需求二选一即可 12345#安装Master节点和Node节点的服务，适用于服务器数量不够时共用同一台服务器yum install kubernetes -y#安装Master节点需要的服务，适用于服务器数量充足分离Master和Node节点yum install kubernetes-master.x86_64 -y 本人Master节点[192.168.1.132]也安装了Node节点服务,可以用于测试两个不同宿主机上Node节点通信。 kubelet默认把数据存放在/var/lib/kubelet下面，如果根目录下空间太小，可能会把磁盘撑爆。可以将数据挂载在充足空间的盘上 1234mkdir -p /home/kubeletcp -r /var/lib/kubelet /home/rm -rf /var/lib/kubeletln -sf /home/kubelet /var/lib/kubelet 4、修改apiserver配置文件 安装好了后进入/etc/kubernetes/配置目录修改相关配置 12345678910111213[root@localhost /]# vim /etc/kubernetes/apiserver#服务的监听地址8 行: KUBE_API_ADDRESS=\"--insecure-bind-address=0.0.0.0\"#服务监听的端口11行：KUBE_API_PORT=\"--port=8080\"#通过10250端口控制kubelet14行：KUBELET_PORT=\"--kubelet-port=10250\" #APIserver是通过那个地址和端口连接etcd数据17行：KUBE_ETCD_SERVERS=\"--etcd-servers=http://192.168.1.132:2379\"#K8S创建service服务的网段配置 20行：KUBE_SERVICE_ADDRESSES=\"--service-cluster-ip-range=10.254.0.0/16\"#默认的管理控制插件---将后面的ServiceAccount去掉23行：KUBE_ADMISSION_CONTROL=\"--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota\" 5、修改通用的配置文件config 123[root@localhost kubernetes]# vim /etc/kubernetes/config#通过那个地址端口找到API服务22行：KUBE_MASTER=\"--master=[http://192.168.1.132:8080] 6、启动服务并设置开启自启 123456systemctl enable kube-apiserver.servicesystemctl start kube-apiserver.servicesystemctl enable kube-controller-manager.servicesystemctl start kube-controller-manager.servicesystemctl enable kube-scheduler.servicesystemctl start kube-scheduler.service 7、测试集群是否正常 12345[root@localhost /]# kubectl get componentstatusNAME STATUS MESSAGE ERRORetcd-0 Healthy &#123;\"health\":\"true\"&#125; controller-manager Healthy ok scheduler Healthy ok 安装Node节点1、Node节点192.168.1.134安装K8S 如果没有第二台服务器，Master节点和Node节点同一台服务器时跳过安装步骤 1yum install kubernetes-node.x86_64 -y (自动会安装docker) 值得一提的是，如果Master节点也安装了Node节点的服务，Master节点机器也需要修改以下的所有相关配置 2、修改kube-proxy服务配置文件 12[root@localhost ~]# vim /etc/kubernetes/config22行：KUBE_MASTER=\"--master=[http://192.168.1.132:8080] 3、修改kubelet服务配置文件 1234567891011[root@localhost ~]# vim /etc/kubernetes/kubelet# 监听的地址5行：KUBELET_ADDRESS=\"--address=0.0.0.0\"#kubelet端口 8行：KUBELET_PORT=\"--port=10250\"# 给自己定义唯一的名字 不能冲突 IP地址或者主机名（各自节点改各自节点的IP）11行：KUBELET_HOSTNAME=\"--hostname-override=192.168.1.134\"# Master节点的连接api的地址14行：KUBELET_API_SERVER=\"--api-servers=[http://192.168.1.132:8080]# 节点的DNS配置KUBELET_ARGS=\"--cluster-dns=192.168.1.1 --cluster-domain=cluster.local\" 如果不知道DNS配置，可以执行下列命令 12345[root@localhost kubernetes]# cat /etc/resolv.conf# Generated by NetworkManagernameserver 192.168.1.1nameserver 114.114.114.114search localdomain 如果没有任何信息显示，vim /etc/resolv.conf并增加以下内容nameserver 114.114.114.114然后把该DNS增加到kubelet文件配置中 4、修改服务通用的配置文件config 123[root@localhost kubernetes]# vim /etc/kubernetes/config#通过那个地址端口找到API服务22行：KUBE_MASTER=\"--master=[http://192.168.1.132:8080] 5、启动服务并设置开机自启 12345systemctl start kubelet.service systemctl enable kubelet.service systemctl start kube-proxy.servicesystemctl enable kube-proxy.service 6、在Master节点测试是否有节点加入集群 1234[root@localhost /]# kubectl get nodeNAME STATUS AGE192.168.1.132 Ready 8d192.168.1.134 Ready 9d 安装flanneld网络通讯由于K8S创建的Service、Pod服务均是生成的虚拟IP，两台Node节点之间的Pod通信需要通过第三方插件实现.Master节点和Node节点都需要安装和配置 1、安装flanneld 1yum install flannel -y 2、修改flanneld配置文件 1sed -i 's#http://127.0.0.1:2379#http://192.168.1.132:2379#g' /etc/sysconfig/flanneld 多网卡的话需要在LANNEL_ETCD_ENDPOINTS项中增加--iface=网卡名 这里/etc/sysconfig/flanneld可以设置密钥验证，详情请自行百度! 3、Master节点配置etcd中关于flanneld的Key 12etcdctl mk /atomic.io/network/config '&#123; \"Network\": \"172.16.0.0/16\" &#125;'etcdctl get /atomic.io/network/config 这里的/atomic.io/network需要和/etc/sysconfig/flanneld里的FLANNEL_ETCD_PREFIX配置对应。 4、设置flanneld启动配置 1234567891011121314151617181920[root@localhost /]# vim /usr/lib/systemd/system/flanneld.service[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyEnvironmentFile=/etc/sysconfig/flanneldEnvironmentFile=-/etc/sysconfig/docker-networkExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONSExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=on-failure[Install]WantedBy=multi-user.targetWantedBy=docker.service 5、设置Docker启动配置 1234567891011121314151617181920212223242526272829303132[root@localhost /]# vim /usr/lib/systemd/system/docker.service[Unit]Description=Docker Application Container EngineDocumentation=http://docs.docker.comAfter=network.targetWants=docker-storage-setup.serviceRequires=docker-cleanup.timer[Service]Type=notifyNotifyAccess=mainEnvironmentFile=/run/flannel/subnet.env #增加该行配置EnvironmentFile=-/etc/sysconfig/dockerEnvironmentFile=-/etc/sysconfig/docker-storageEnvironmentFile=-/etc/sysconfig/docker-networkEnvironment=GOTRACEBACK=crashEnvironment=DOCKER_HTTP_HOST_COMPAT=1Environment=PATH=/usr/libexec/docker:/usr/bin:/usr/sbin#更改启动方式 如果没有更改过docker存储路径，不需要配置 --graph /home/dockerExecStart=/usr/bin/dockerd --graph /home/docker $DOCKER_NETWORK_OPTIONS#增加docker启动的防火墙拦截允许配置ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPTExecReload=/bin/kill -s HUP $MAINPIDLimitNOFILE=1048576LimitNPROC=1048576LimitCORE=infinityTimeoutStartSec=0Restart=on-abnormalKillMode=process[Install]WantedBy=multi-user.target 6、启动flanneld 12345678910111213141516systemctl daemon-reloadsystemctl start flanneld.servicesystemctl enable flanneld.service#重启Master节点服务systemctl restart dockersystemctl restart kube-apiserversystemctl restart kube-controller-managersystemctl restart kube-schedulersystemctl restart kubeletsystemctl restart kube-proxy#重启Node节点服务systemctl restart dockersystemctl restart kubeletsystemctl restart kube-proxy 7、测试网段 以上的一系列操作，主要是为了让flanneld和docker创建的网络处于同一个网段，先通过设置etcd设置flanneld的网段范围，再配置docker启动前加载flanneld的配置从图中能够看到docker0和flannel0都处于同一个网段172.16.0.0中,可以用一个轻巧的容器测试一下 1234#docker拉取网络镜像[root@localhost /]# docker pull docker.io/busybox#分别在Master节点及Node节点执行命令[root@localhost /]# docker run -it docker.io/busybox:latest 从图中可以看到，创建了两个容器，IP分别是172.16.43.6和172.16.43.4互相能够Ping通。 如果不能Ping通，则在每个Node节点上都配置相应的静态路由项 1234#Master节点 #172.16.9.0 为Node节点的flannel0的IP[root@localhost ~]# route add -net 172.16.9.0 netmask 255.255.255.0 gw 192.168.1.134 #Node节点 #172.16.43.0 为Master节点的flannel0的IP[root@localhost ~]# route add -net 172.16.43.0 netmask 255.255.255.0 gw 192.168.1.132 这意味着，每一个新部署的容器都将使用这个Node（docker0的网桥IP）作为它的默认网关。而这些Node（类似路由器）都有其他docker0的路由信息，这样它们就能够相互连通了。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://midkuro.github.io/categories/Kubernetes/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://midkuro.github.io/tags/Kubernetes/"}]},{"title":"'kubernetes 基础概念'","slug":"kubernetes-introduction","date":"2020-05-21T13:50:00.000Z","updated":"2020-05-21T12:32:26.834Z","comments":true,"path":"2020/05/21/kubernetes-introduction/","link":"","permalink":"https://midkuro.github.io/2020/05/21/kubernetes-introduction/","excerpt":"","text":"Docker基本概念什么是DockerDocker是使用 Google公司推出的Go 语言进行开发实现，基于Linux 内核的cgroup、namespace、以及AUFS类的Union FS等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术，由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。 Docker在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护。使得Docker技术比虚拟机技术更为轻便、快捷。下面的图片比较了Docker和传统虚拟化方式的不同之处。 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程。 而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比 传统虚拟机更为轻便。 为什么要使用Docker作为一种新兴的虚拟化方式，Docker跟传统的虚拟化方式相比具有众多的优势。 更高效的利用系统资源：由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker对系统资源的利用率更高。 更快速的启动时间： Docker容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间 一致的运行环境： Docker的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现这段代码在我机器上没问题啊这类问题。 持续交付和部署： Docker可以通过定制应用镜像来实现持续集成、持续交付、部署。可以通过Dockerfile来进行镜像构建，结合持续集成系统进行集成测试、自动部署。 更轻松的迁移： 由于Docker确保了执行环境的一致性，使得应用的迁移更加容易，而不用担心运行环境的变化导致应用无法正常运行的情况。 更轻松的维护和扩展： Docker使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。此外，Docker团队提供了一大批高质量的官方镜像，既可以直接使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。 特性 容器 虚拟机 启动 秒级 分钟级 硬盘使用 一般为 MB 一般为 GB 性能 接近原生 弱于 系统支持量 单机支持上千个容器 一般几十个 基本概念镜像Image 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像 不包含任何动态数据，其内容在构建之后也不会被改变。 Docker设计时，将其设计为分层存储的架构。镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除 前一层的文件，而是仅在当前层标记为该文件已删除。分层存储的特征使得镜像的复用、定制变的更为容易。 容器Container 镜像和容器的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。因此容器可以拥有自己的root文件系统、自己的网络配置、自己的进程空间，甚至自己的用户ID空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。容器也具备分层存储的特征。 Kubernetes什么是KubernetesKubernetes是一个完备的分布式系统支撑平台。Kubernetes具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建的智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。同时，Kubernetes提供了完善的管理工具，这些工具涵盖了包括开发、部署测试、运维监控在内的各个环节。因此，Kubernetes是一个全新的基于容器技术的分布式架构解决方案，并且是一个一站式的完备的分布式系统开发和支撑平台。 Kubernetes英文中字母K和S中间有8个英文，所以也简称为K8S。 为什么要使用Kubernetes 轻装上阵地开发复杂系统 可以全面拥抱微服务架构 可以随时随地迁移系统 拥有横向弹性扩容机制 Kubernetes基本概念Kubernetes中的大部分概念如Node、Pod、Replication Controller、Service等都可以被看作一种资源对象，几乎所有资源对象都可以通过Kubernetes提供的kubectl工具（或者API编程调用）执行增、删、改、查等操作并将其保存在etcd中持久化存储。从这个角度来看，Kubernetes其实是一个高度自动化的资源控制系统，它通过跟踪对比etcd库里保存的“资源期望状态”与当前环境中的“实际资源状态”的差异来实现自动控制和自动纠错的高级功能。 MasterKubernetes里的Master指的是集群控制节点，在每个Kubernetes集群里都需要有一个Master来负责整个集群的管理和控制，基本上Kubernetes的所有控制命令都发给它，它负责具体的执行过程，我们后面执行的所有命令基本都是在Master上运行的。Master通常会占据一个独立的服务器（高可用部署建议用3台服务器），主要原因是它太重要了，是整个集群的“首脑”，如果它宕机或者不可用，那么对集群内容器应用的管理都将失效。 在Master上运行着以下关键进程: Kubernetes API Server（kube-apiserver） 提供了HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程。 Kubernetes Controller Manager（kube-controller-manager） Kubernetes里所有资源对象的自动化控制中心，可以将其理解为资源对象的大总管。 Kubernetes Scheduler（kube-scheduler） 负责资源调度（Pod调度）的进程，相当于公交公司的调度室，在Master上通常还需要部署etcd服务，因为Kubernetes里的所有资源对象的数据都被保存在etcd中。 Node除了Master，Kubernetes集群中的其他机器被称为Node，在较早的版本中也被称为Minion。与Master一样，Node可以是一台物理主机，也可以是一台虚拟机。Node是Kubernetes集群中的工作负载节点，每个Node都会被Master分配一些工作负载（Docker容器），当某个Node宕机时，其上的工作负载会被Master自动转移到其他节点上。 在每个Node上都运行着以下关键进程: kubelet 负责Pod对应的容器的创建、启停等任务，同时与Master密切协作，实现集群管理的基本功能。 kube-proxy 实现Kubernetes Service的通信与负载均衡机制的重要组件。 Docker Engine（docker） Docker引擎，负责本机的容器创建和管理工作。 Node可以在运行期间动态增加到Kubernetes集群中，前提是在这个节点上已经正确安装、配置和启动了上述关键进程，在默认情况下kubelet会向Master注册自己，这也是Kubernetes推荐的Node管理方式。 一旦Node被纳入集群管理范围，kubelet进程就会定时向Master汇报自身的情报，例如操作系统、Docker版本、机器的CPU和内存情况，以及当前有哪些Pod在运行等，这样Master就可以获知每个Node的资源使用情况，并实现高效均衡的资源调度策略。 而某个Node在超过指定时间不上报信息时，会被Master判定为“失联”，Node的状态被标记为不可用（Not Ready），随后Master会触发“工作负载大转移”的自动流程。 我们可以执行下述命令查看在集群中有多少个Node： 然后可以通过kubectl describe node &lt;node_name&gt;查看某个Node的详细信息: PodPod是Kubernetes最重要的基本概念，下图所示是Pod的组成示意图，我们看到每个Pod都有一个特殊的被称为根容器的Pause容器。Pause容器对应的镜像属于Kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或多个紧密相关的用户业务容器。 为什么Kubernetes会设计出一个全新的Pod的概念并且Pod有这样特殊的组成结构？ 原因之一：在一组容器作为一个单元的情况下，我们难以简单地对“整体”进行判断及有效地行动。比如，一个容器死亡了，此时算是整体死亡么？是N/M的死亡率么？引入业务无关并且不易死亡的Pause容器作为Pod的根容器，以它的状态代表整个容器组的状态，就简单、巧妙地解决了这个难题。 原因之二：Pod里的多个业务容器共享Pause容器的IP，共享Pause容器挂接的Volume，这样既简化了密切关联的业务容器之间的通信问题，也很好地解决了它们之间的文件共享问题。 Kubernetes为每个Pod都分配了唯一的IP地址，称之为Pod IP，一个Pod里的多个容器共享Pod IP地址。Kubernetes要求底层网络支持集群内任意两个Pod之间的TCP/IP直接通信，这通常采用虚拟二层网络技术来实现，例如Flannel、Open vSwitch等 因此我们需要牢记一点：在Kubernetes里，一个Pod里的容器与另外主机上的Pod容器能够直接通信，同一个Pod里的容器之间仅需通过localhost就能互相通信。 Pod、容器与Node的关系: 一个Pod中的应用容器共享同一组资源： PID命名空间：Pod中的不同应用程序可以看到其他应用程序的进程ID 网络命名空间：Pod中的多个容器能够访问同一个IP和端口范围 IPC命名空间：Pod中的多个容器能够使用SystemV IPC或POSIX消息队列进行通信 UTS命名空间：Pod中的多个容器共享一个主机名 Volumes（共享存储卷）：Pod中的各个容器可以访问在Pod级别定义的Volumes Pod的生命周期通过Replication Controller来管理；通过模板进行定义，然后分配到一个Node上运行，在Pod所包含容器运行结束后，Pod结束。 Kubernetes为Pod设计了一套独特的网络配置，包括：为每个Pod分配一个IP地址，使用Pod名作为容器间通信的主机名等。 LabelLabel（标签）是Kubernetes系统中另外一个核心概念。一个Label是一个key=value的键值对，其中key与value由用户自己指定。Label可以被附加到各种资源对象上，例如Node、Pod、Service、RC等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上。Label通常在资源对象定义时确定，也可以在对象创建后动态添加或者删除。 我们可以通过给指定的资源对象捆绑一个或多个不同的Label来实现多维度的资源分组管理功能，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。 Label相当于我们熟悉的“标签”。给某个资源对象定义一个Label，就相当于给它打了一个标签，随后可以通过Label Selector（标签选择器）查询和筛选拥有某些Label的资源对象，Kubernetes通过这种方式实现了类似SQL的简单又通用的对象查询机制。 Service在Kubernetes的世界里，虽然每个Pod都会被分配一个单独的IP地址，但这个IP地址会随着Pod的销毁而消失，这就引出一个问题：如果有一组Pod组成一个集群来提供服务，那么如何来访问它呢？Service！ 一个Service可以看作一组提供相同服务的Pod的对外访问接口，Service作用于哪些Pod是通过Label Selector来定义的。 拥有一个指定的名字（比如my-mysql-server） 拥有一个虚拟IP（Cluster IP、Service IP或VIP）和端口号，销毁之前不会改变，只能内网访问 能够提供某种远程服务能力 被映射到了提供这种服务能力的一组容器应用上 如果Service要提供外网服务，需指定公共IP和NodePort，或外部负载均衡器 Service可以通过配置NodePort，在Node上打开一个主机的真实端口，这样，能够访问Node的客户端就能通过这个端口访问到内部的Service了 Replication ControllerReplication Controller（简称RC）是Kubernetes系统中的核心概念之一，简单来说，它其实定义了一个期望的场景，即声明某种Pod的副本数量在任意时刻都符合某个预期值 目标Pod的定义 目标Pod需要运行的副本数量 要监控的目标Pod标签（Label） Kubernetes通过RC中定义的Label筛选出对应的Pod实例，并实时监控其状态和数量，如果实例数量少于定义的副本数量（Replicas），则会根据RC中定义的Pod模板来创建一个新的Pod，然后将此Pod调度到合适的Node上启动运行，直到Pod实例数量达到预定目标。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://midkuro.github.io/categories/Kubernetes/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://midkuro.github.io/tags/Kubernetes/"}]},{"title":"'docker 安装私服仓库'","slug":"docker-registry","date":"2020-05-21T13:00:00.000Z","updated":"2020-05-21T12:15:54.606Z","comments":true,"path":"2020/05/21/docker-registry/","link":"","permalink":"https://midkuro.github.io/2020/05/21/docker-registry/","excerpt":"","text":"docker私服仓库docker的私服仓库存储不像maven私服有完整独立的应用，它是通过docker获取私服仓库镜像，并根据镜像创建私服仓库容器 说白了，docker的私服仓库的搭建就是拉取镜像、创建容器、上传镜像的过程 宿主机环境IP：192.168.1.131 私服仓库的搭建1、拉取私服镜像 1docker pull registry:2 2、启动私服 1docker run --name registry -tid --privileged=true --restart=always --net=host -v /home/docker/repository:/var/lib/registry registry 这里使用的是V2版本的私服仓库,/var/lib/registry是私服仓库存储上传镜像的路径，把它挂载到宿主机上避免因为容器坏损丢失私服仓库的镜像存储 3、标记镜像 12命令：docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]例子：docker tag mtex-admin 192.168.1.131:5000/mtex-admin 这里以镜像名：mtex-admin为例，标记镜像将其归入某一仓库,再次查看镜像列表 1docker images 能够看见出现了192.168.1.131:5000/mtex-admin镜像名称，细心点会发现它的镜像ID和mtex-admin是一样的也就是说，两个镜像名称都映射到同一个镜像ID上，如何避免这种情况呢？ 可以执行tag命令之后，删除原来的旧镜像名称，只保留一个名称映射 也可以在创建镜像时，镜像名称以[私服IP:端口/名称]命名,不必在执行tag命令 4、上传私服 1docker push 192.168.1.131:5000/mtex-admin 可以看到上传成功了，使用192.168.1.134试一下拉取镜像 1docker pull 192.168.1.131:5000/mtex-admin 5、配置解析 考虑到记住IP比较麻烦，可以在/etc/hosts中增加本地仓库的域名解析 1echo \"192.168.1.131 docker-registry\" &gt;&gt; /etc/hosts 这时候再执行cat /etc/hosts能够看到，已经增加进去了 6、查看私服镜像 可以通过浏览器打开http://192.168.1.131:5000/v2/_catalog查看 7、删除私服镜像 上面已经将私服的镜像内容挂载到宿主机/home/docker/repository路径中，只需要进入对应路径删除镜像即可路径目录是/home/docker/repository/docker/registry/v2/repositories 常见问题Docker自从1.3.X之后docker registry交互默认使用的是HTTPS，但是搭建私有镜像默认使用的是HTTP服务，所以与私有镜像交互时可能会出现以下错误 1Get https://192.168.1.131:5000/v2/: http: server gave HTTP response to HTTPS client docker版本1.2以上的，在/etc/docker/daemon.json文件中增加以下内容 12#必须要增加在第一行&#123; \"insecure-registries\":[\"192.168.1.131:5000\"] 然后重启docker，重启registry 1systemctl restart docker.service 查看docker版本的命令docker -v，低于1.2的版本可以升级版本，也可以上网寻找解决方法.","categories":[{"name":"Docker","slug":"Docker","permalink":"https://midkuro.github.io/categories/Docker/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Docker","slug":"Docker","permalink":"https://midkuro.github.io/tags/Docker/"}]},{"title":"'docker 执行DockerFile创建镜像'","slug":"docker-dockfile","date":"2020-05-21T12:30:00.000Z","updated":"2020-05-21T12:16:03.029Z","comments":true,"path":"2020/05/21/docker-dockfile/","link":"","permalink":"https://midkuro.github.io/2020/05/21/docker-dockfile/","excerpt":"","text":"DockerFile 文件常用详解 FROM：指定基础镜像，必须为第一个命令 12345678格式： FROM &lt;image&gt; FROM &lt;image&gt;:&lt;tag&gt; FROM &lt;image&gt;@&lt;digest&gt;示例： FROM centos:7.2.1511注： tag或digest是可选的，如果不使用这两个值时，会使用latest版本的基础镜像 MAINTAINER：维护者信息 123456格式： MAINTAINER &lt;name&gt;示例： MAINTAINER caijinkun MAINTAINER caijinkun &lt;caijinkun@mastercom.cn&gt; MAINTAINER caijinkun \"caijinkun@mastercom.cn\" RUN：构建镜像时执行的命令 123456789101112131415RUN用于在镜像容器中执行命令，其有以下两种命令执行方式：shell执行格式： RUN &lt;command&gt;exec执行格式： RUN [\"executable\", \"param1\", \"param2\"]示例： RUN [\"executable\", \"param1\", \"param2\"] RUN apk update RUN [\"/etc/execfile\", \"arg1\", \"arg1\"]注： RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像， 可以在构建时指定--no-cache参数,如：docker build --no-cache 每执行Run命令，都会创建一个中间镜像层，使得最终创建的镜像文件变大，建议把命令都在一个Run中执行，用&amp;&amp;分隔. ADD：将本地文件添加到容器中，tar类型文件会自动解压(网络压缩资源不会被解压)，可以访问网络资源，类似wget 12345678910格式： ADD &lt;src&gt;... &lt;dest&gt; ADD [\"&lt;src&gt;\",... \"&lt;dest&gt;\"] 用于支持包含空格的路径示例： ADD hom* /mydir/ # 添加所有以\"hom\"开头的文件 ADD hom?.txt /mydir/ # ? 替代一个单字符,例如：\"home.txt\" ADD test relativeDir/ # 添加 \"test\" 到 `WORKDIR`/relativeDir/ ADD test /absoluteDir/ # 添加 \"test\" 到 /absoluteDir/注： 该命令只能添加Dockerfile路径的下层文件，所以需要事先拷贝文件到该路径下 COPY：功能类似ADD，但是是不会自动解压文件，也不能访问网络资源，建议直接用ADD CMD：构建容器后调用，也就是在容器启动时才进行调用 12345678910格式： CMD [\"executable\",\"param1\",\"param2\"] (执行可执行文件，优先) CMD [\"param1\",\"param2\"] (设置了ENTRYPOINT，则直接调用ENTRYPOINT添加参数) CMD command param1 param2 (执行shell内部命令)示例： CMD echo \"This is a test.\" | wc - CMD [\"/usr/bin/wc\",\"--help\"] CMD [\"java\",\"-jar\",\"mtex-config-0.0.1-SNAPSHOT.jar\"]注： CMD不同于RUN，CMD用于指定在容器启动时所要执行的命令，而RUN用于指定镜像构建时所要执行的命令 ENTRYPOINT：配置容器，使其可执行化。配合CMD可省去”application”，只使用参数。 1234567891011格式： ENTRYPOINT [\"executable\", \"param1\", \"param2\"] (可执行文件, 优先) ENTRYPOINT command param1 param2 (shell内部命令)示例： FROM ubuntu ENTRYPOINT [\"top\", \"-b\"] CMD [\"-c\"]注： ENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖ENTRYPOINT 而docker run命令中指定的任何参数，都会被当做参数再次传递给ENTRYPOINT Dockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，而只执行最后的ENTRYPOINT指令。 ENV：设置环境变量 1234567格式： ENV &lt;key&gt; &lt;value&gt; #&lt;key&gt;之后的所有内容均会被视为其&lt;value&gt;的组成部分，因此，一次只能设置一个变量 ENV &lt;key&gt;=&lt;value&gt; ... #可以设置多个变量，每个变量为一个\"&lt;key&gt;=&lt;value&gt;\"的键值对，如果&lt;key&gt;中包含空格，可以使用\\来进行转义，也可以通过\"\"来进行标示；另外，反斜线也可以用于续行示例： ENV myName John Doe ENV myDog Rex The Dog ENV myCat=fluffy EXPOSE：指定于外界交互的端口 12345678910格式： EXPOSE &lt;port&gt; [&lt;port&gt;...]示例： EXPOSE 80 443 EXPOSE 8080 EXPOSE 11211/tcp 11211/udp注： EXPOSE并不会让容器的端口访问到主机。要使其可访问 需要在docker run 运行容器时通过 -p 来发布这些端口 或通过 -P 参数来发布EXPOSE导出的所有端口 VOLUME：用于指定持久化目录 123456789101112格式： VOLUME [\"path\"]示例： VOLUME [\"/data\",\"/home\"]注： 和docker run -v 的区别是无法挂载指定的目录. 一个卷可以存在于一个或多个容器的指定目录，该目录可以绕过联合文件系统，并具有以下功能： 1.卷可以容器间共享和重用 2.容器并不一定要和其它容器共享卷 3.修改卷后会立即生效 4.对卷的修改不会对镜像产生影响 5.卷会一直存在，直到没有任何容器在使用它 WORKDIR：工作目录，类似于cd命令 12345678格式： WORKDIR path示例： WORKDIR /home (这时工作目录为/home) WORKDIR jenkins-running-jar (这时工作目录为/home/jenkins-running-jar)注： 通过WORKDIR设置工作目录后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT、ADD、COPY等命令都会在该目录下执行。 在使用docker run运行容器时，可以通过-w参数覆盖构建时所设置的工作目录。 docker 执行DockerFile创建镜像基础镜像 docker pull centos:7.2.1511先从仓库中拉取一个centos7.2的系统，来作为最底层的镜像 先下载 jre-8u201-linux-x64.tar.gz 下载地址 以此为基础创建一个具备java环境的基础镜像 这里使用jre而非jdk，主要是为了降低镜像大小。 这里注意需要引入镜像的文件必须与Dockerfile路径同级或下级 在jre-8u201-linux-x64.tar.gz文件路径下 vi Dockerfile 创建镜像，拷贝以下内容 123456789101112131415#使用的基础镜像FROM centos:7.2.1511#作者信息MAINTAINER caijinkun \"caijinkun@mastercom.cn\"#安装中文和zip支持RUN yum -y install kde-l10n-Chinese &amp;&amp; yum -y reinstall glibc-common &amp;&amp; yum -y install unzip zip &amp;&amp; yum clean all &amp;&amp; localedef -c -f UTF-8 -i zh_CN zh_CN.utf8#加入jreADD jre-8u201-linux-x64.tar.gz /usr/local/#设置环境变量ENV JAVA_HOME /usr/local/jre1.8.0_201ENV PATH $JAVA_HOME/bin:$PATHENV LC_ALL zh_CN.utf8ENV TZ Asia/Shanghai 执行Dockfile文件 docker build -t java:jre1.8.0.201 . 并检查镜像创建是否成功docker images java mtex-config镜像基础镜像已经完毕，准备创建程序的DockerFile和程序，这里以mtex-config为例 12345678910111213#使用的基础镜像FROM java:jre1.8.0.201#作者信息MAINTAINER caijinkun \"caijinkun@mastercom.cn\"#加入服务ADD mtex-config-0.0.1-SNAPSHOT.zip tmp.zipRUN mkdir -p /home/jenkins-running-jar/mtex-config/ &amp;&amp; unzip -o tmp.zip -d /home/jenkins-running-jar/mtex-config/ &amp;&amp; rm -f tmp.zip &amp;&amp; rm -rf /home/jenkins-running-jar/mtex-config/properties &amp;&amp;rm -rf /home/jenkins-running-jar/mtex-config/properties_*#工作目录WORKDIR /home/jenkins-running-jar/mtex-config/#启动服务CMD [\"nohup\",\"java\",\"-jar\",\"mtex-config-0.0.1-SNAPSHOT.jar\"] 这里需要引入自身的配置文件夹，所以把压缩包引入镜像后先删除原配置文件夹.待使用docker启动时将自身配置文件挂载进去，也可以使用其他方式. 使用docker启动mtex-config,这里pro-properties是我自身的配置文件夹名称，这里挂载到主机的是linux自带的日志，可自行进行log日志挂载 1docker run --name mtex-config -tid --privileged=true --restart=always --net=host -v /home/jenkins-running-jar/mtex-config/pro-properties:/home/jenkins-running-jar/mtex-config/properties/ -v /home/file/nohupLog/mtex-config.log:/home/jenkins-running-jar/mtex-config/nohup.out mtex-config 启动容器时把自身的配置文件夹挂载到容器中即可.--net=host代表和主机使用同一个网段,即同一个IP和端口. 如果打算通过-p进行端口映射，需要先在启宿主机的防火墙上开启该端口，并对外暴露. mtex-sys镜像程序镜像基本都类似，都是根据业务要求挂载不同的文件即可. 12345678910111213#使用的基础镜像FROM java:jre1.8.0.201#作者信息MAINTAINER caijinkun \"caijinkun@mastercom.cn\"#加入服务ADD mtex-sys-0.0.1-SNAPSHOT.zip tmp.zipRUN mkdir -p /home/jenkins-running-jar/mtex-sys/ &amp;&amp; unzip -o tmp.zip -d /home/jenkins-running-jar/mtex-sys/ &amp;&amp; rm -f tmp.zip#工作目录WORKDIR /home/jenkins-running-jar/mtex-sys/#启动服务CMD [\"nohup\",\"java\",\"-jar\",\"mtex-sys-0.0.1-SNAPSHOT.jar\"] docker启动mtex-sys实例: 1docker run --name mtex-sys -tid --privileged=true --restart=always --net=host -v /home/file/nohupLog/mtex-sys.log:/home/jenkins-running-jar/mtex-sys/nohup.out mtex-sys 这时要注意，sys作为框架需要访问很多个项目的文件夹配置及路径，根据需求进行挂载，或者弄一个项目的共享文件夹即可.","categories":[{"name":"Docker","slug":"Docker","permalink":"https://midkuro.github.io/categories/Docker/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Docker","slug":"Docker","permalink":"https://midkuro.github.io/tags/Docker/"}]},{"title":"'docker 安装nginx redis'","slug":"docker-use","date":"2020-05-21T11:30:00.000Z","updated":"2020-05-21T12:16:07.838Z","comments":true,"path":"2020/05/21/docker-use/","link":"","permalink":"https://midkuro.github.io/2020/05/21/docker-use/","excerpt":"","text":"docker 安装 nginxdocker安装镜像，使用的是两个命令docker search xxx 和 docker pull xxx，xxx则为要安装的镜像名称,这里要注意的是创建容器会和主机时间相差8个小时 搜索nginx镜像 docker search nginx 安装nginx镜像 docker pull nginx 查看镜像信息 docker images nginx 建议在nginx.conf中配置使用root启动nginx避免权限不足引起问题 编辑配置文件并设置为root. 重命名镜像名称 docker tag IMAGEID REPOSITORY:TAG IMAGEID是镜像ID，REPOSITORY是镜像新名称，TAG是镜像新标签 创建并启动容器命令 docker run [OPTIONS] IMAGE [COMMAND] [ARG...] docker启动nginx实例： docker run 参数配置 作用 --name xxx 为容器指定一个名称为 xxx -t 为容器重新分配一个伪输入终端，通常与 -i 同时使用； -i 以交互模式运行容器，通常与 -t 同时使用； -d 后台运行容器，并返回容器ID； --privileged=true centos默认关闭SElinux，需要开启特权模式，以root的形式进入容器，否则是普通用户 --restart=always 容器开启自动启动 -p 端口映射，格式为：主机(宿主)端口:容器端口 -v 主机路径:容器路径 把主机的文件挂载到容器中 或 把容器文件同步到主机中 -v /etc/localtime:/etc/localtime:ro 把主机时间同步到容器中，不同步会相差8小时 123456789docker run --name nginx -tid --privileged=true --restart=always -p 8181:8181 -v /home/local/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /home/file/log/nginx/log:/var/log/nginx -v /home/jenkins-running-jar/static/:/home/jenkins-running-jar/static/ -v /etc/localtime:/etc/localtime:ro docker.io/nginx#分解命令#把容器的日志配置同步到主机中-v /home/file/log/nginx/log:/var/log/nginx #是把主机中的配置文件挂载到容器中-v /home/local/nginx/conf/nginx.conf:/etc/nginx/nginx.conf#挂载nginx配置中需要访问的静态文件-v /home/jenkins-running-jar/static/:/home/jenkins-running-jar/static/ docker.io/nginx为镜像名称 查看容器命令 docker ps -a 停止容器命令 docker stop 容器ID或容器名称 删除容器命令 docker rm 容器ID或者容器名称 进入容器命令 docker exec -it 容器ID /bin/bash 退出容器命令 exit 删除镜像命令 docker rmi 镜像ID docker 安装 Redis安装Redis镜像 docker pull redis:3.2 准备 redis.conf,若无此文件可自行新建同名文件并复制进去如果使用上文配置文件则可不需要执行以下操作，原版的redis.conf需修改以下几点:原文件： 1234bind 127.0.0.1protected-mode yesappendonly no//持久化# requirepass foobared 修改后： 1234#bind 127.0.0.1protected-mode noappendonly yes//持久化requirepass yourpassword //redis密码 docker启动redis实例： 1234567docker run --name redis -tid --privileged=true --restart=always -p 6379:6379 -v /home/local/redis/redis.conf:/etc/redis/redis.conf -v /home/local/redis/data:/data redis redis-server /etc/redis/redis.conf#分解命令#把主机的配置文件挂载到容器中-v /home/local/redis/redis.conf:/etc/redis/redis.conf#映射挂载的数据目录-v /home/local/redis/data:/data","categories":[{"name":"Docker","slug":"Docker","permalink":"https://midkuro.github.io/categories/Docker/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Docker","slug":"Docker","permalink":"https://midkuro.github.io/tags/Docker/"}]},{"title":"'docker 安装及存储路径配置'","slug":"docker-install","date":"2020-05-21T11:00:00.000Z","updated":"2020-05-21T12:15:59.606Z","comments":true,"path":"2020/05/21/docker-install/","link":"","permalink":"https://midkuro.github.io/2020/05/21/docker-install/","excerpt":"","text":"docker 安装及存储路径配置docker的安装安装docker yum install docker 启动服务 systemctl start docker.service 设置开机自启 systemctl enable docker.service 查看docker版本 docker version 使用centos7系统的，建议把selinux服务关闭，它会影响docker的启动及一些容器的使用. SELinux一共有3种状态，分别是Enforcing，Permissive和Disabled状态。第一种是默认状态，表示强制启用，第二种是宽容的意思，即大部分规则都放行。第三种是禁用，即不设置任何规则。可以通过命令 getenforce 查看selinux服务的状态，默认一般都是 Enforcing。 修改selinux状态，编辑配置文件 vi /etc/selinux/config 将 SELINUX=enforcing改为SELINUX=disabled，该操作后需要重启机器。 docker存储路径配置docker每次创建一个镜像、容器都会占据大量的内存空间，所以建议在安装的时候就把docker放在大空间的路径下，默认的docker是安装在/var/lib/docker下. 如果docker已经启动，请先关闭它 systemctl stop docker.service . 我打算把它迁移到 /home/docker 路径下，所以先创建文件夹 mkdir /home/docker 然后编辑docker存储路径配置 vi /lib/systemd/system/docker.service,找到ExecStart项 ,把它改成ExecStart=/usr/bin/dockerd --graph /home/docker 保存文件后需要执行命令重新加载配置 systemctl daemon-reload. 然后将原路径文件拷贝到新的路径下 cp -r /var/lib/docker/* /home/docker/ docker中不存在容器的话，可以把原路径下的文件删除 如果docker已经存在容器，需要迁移容器的挂载点 首先输入命令 df -hl 查看挂载点，docker容器会创建名称为overlay和shm的挂载点.这时候可以执行命令cat /proc/mounts|grep docker查看挂载点哪些是属于docker的. 然后执行 umount 挂载点全路径 去掉该容器的挂载，并修改挂载点的路径，改成迁移后的docker路径,重新执行挂载命令 mount 新挂载点路径 再次查看挂载 df -hl 这时候应该看到的都是新的路径，也可以删除原路径下的文件了，如果依旧删除失败，重启机器后可删除，然后尝试运行镜像。 如果运行镜像失败提示:Error response from daemon: shim error: docker-runc not installed on system,执行命令创建软连接cd /usr/libexec/docker/和ln -s docker-runc-current docker-runc 如果运行镜像失败提示:exec: “docker-proxy”: executable file not found in $PATH，执行命令创建软连接 ln -s /usr/libexec/docker/docker-proxy-current /usr/bin/docker-proxy 再次运行镜像，应该就能启动了，个人觉得迁移存储路径，有很大可能会出现上面的运行镜像失败的错误，如果是新装的docker迁移了存储路径，也可以先执行上面创建软连接的命令避免以后发生错误. 常用的docker命令 docker run 参数配置 作用 -v /etc/localtime:/etc/localtime:ro 把主机时间同步到容器中，不同步会相差8小时 --privileged=true centos默认关闭SElinux，需要开启特权模式，以root的形式进入容器，否则是普通用户 --restart=always 容器开启自动启动 --net=host 容器和宿主机的IP同网段 -v 主机路径:容器路径 用户把主机的文件挂载到容器中 /usr/sbin/init 在容器中开启系统命令，能够使用systemctl的命令 -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -p: 端口映射，格式为：主机(宿主)端口:容器端口 -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； --name=nginx-lb 为容器指定一个名称； 命令大全","categories":[{"name":"Docker","slug":"Docker","permalink":"https://midkuro.github.io/categories/Docker/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Docker","slug":"Docker","permalink":"https://midkuro.github.io/tags/Docker/"}]},{"title":"'linux 安装Nodejs'","slug":"nodeJs-install","date":"2020-05-20T13:00:00.000Z","updated":"2020-05-20T13:22:18.317Z","comments":true,"path":"2020/05/20/nodeJs-install/","link":"","permalink":"https://midkuro.github.io/2020/05/20/nodeJs-install/","excerpt":"","text":"node.js安装第一种安装方式1.安装gcc，make，openssl 1yum install -y gcc make gcc-c++ openssl-devel 2.下载安装包下载 node-v9.3.0-linux-x64.tar.gz 下载地址 需要其他版本的请到官网中下载即可 官网地址 3.上传安装包 创建nodejs路径文件夹 1mkdir /var/local/nodejs 进入该路径,并上传安装包到该路径中 1cd /var/local/nodedjs 4.解压安装包 1tar -xf node-v9.3.0-linux-x64.tar.gz 5.编译 进入源代码所在路径 1cd node-v9.3.0-linux-x64 执行配置脚本 1./configure 编译与部署 1make &amp;&amp; make install 6.测试 12node -vnpm -v 这种方式安装，需要安装安装gcc等一些编译环境插件，而且编译比较久，部署完成后nodejs为分别放在好几个文件夹内： 123456#放置nodejs 执行程序/usr/local/bin#放置了node_modules，即nodejs的各种模块/usr/lib#放置了nodejs扩展开发用头文件/usr/include 优点是全局安装nodejs模块，直接使用，而且不受用户访问权限影响，推荐使用这种. 第二种安装方式可以不用执行上面的第一步操作，然后用以下方式替代第五步操作 确认node.js的路径，我这里是/usr/local/nodejs/node-v9.3.0-linux-x64/bin，依次执行 12ln -s /usr/local/nodejs/node-v9.3.0-linux-x64/bin/node /usr/bin/nodeln -s /usr/local/nodejs/node-v9.3.0-linux-x64/bin/npm /usr/bin/npm 注意ln指令用于创建关联（类似与Windows的快捷方式）必须给全路径，否则可能关联错误 该方式需要使用root权限去关联，并且非root用户需要做环境变量配置才能使用node.js node.js卸载1.自带工具删除 1yum remove nodejs npm -y 2.2.手动删除残留 进入 /usr/local/bin 删除 node 的可执行文件node和npm 进入 /usr/local/lib 删除所有 node 和 node_modules文件夹 进入 /usr/local/include 删除所有 node 和 node_modules 文件夹 检查 ~ 文件夹里面的”local”、”lib”、”include”、文件夹，然后删除里面的所有”node” 和”node_modules”文件夹 jenkins中使用node.js 在jenkins界面上 系统管理-全局工具配置 中配置安装的nodejs路径 先搭建一个jenkins前端构建任务，构建一次，作用是为了让jenkins检出SVN上的前端代码 到jenkins项目路径中 cd进入workspace文件夹，再进入前端任务名称的文件夹 确认检出的SVN代码文件夹中是否有package.json文件,进入文件路径中 执行以下命令安装node_modules 123npm install webpack -gnpm install webpack-cli -gnpm install --unsafe-perm=true --allow-root 然后组件安装完成后，即可在jenkins构建任务中编辑shell命令执行npm run dist-p-xxx等操作","categories":[{"name":"Nodejs","slug":"Nodejs","permalink":"https://midkuro.github.io/categories/Nodejs/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Nodejs","slug":"Nodejs","permalink":"https://midkuro.github.io/tags/Nodejs/"}]},{"title":"'linux 下安装nginx redis'","slug":"software-install","date":"2020-05-20T12:35:00.000Z","updated":"2020-05-20T15:48:59.355Z","comments":true,"path":"2020/05/20/software-install/","link":"","permalink":"https://midkuro.github.io/2020/05/20/software-install/","excerpt":"","text":"安装nginx1.下载安装包nginx-1.14.1.tar.gz 其他版本请自行下载 官网地址 2.上传并解压安装包 1tar -zxvf nginx-1.14.1.tar.gz 3.设置配置信息 1./configure --prefix=/usr/local/nginx (安装后的文件存放路径） 如果出现以下异常信息 1234./configure: error: the HTTP rewrite module requires the PCRE library.You can either disable the module by using --without-http_rewrite_moduleoption, or install the PCRE library into the system, or build the PCRE librarystatically from the source with nginx by using --with-pcre=&lt;path&gt; option. 则执行命令安装pcre-devel 1yum -y install pcre-devel 安装pcre-devel完成后再次执行命令 1./configure --prefix=/usr/local/nginx 执行完后还有可能会出现这样的问题： 1234567checking for PCRE JIT support ... not foundchecking for system md library ... not foundchecking for system md5 library ... not foundchecking for OpenSSL md5 crypto library ... not foundchecking for sha1 in system md library ... not foundchecking for OpenSSL sha1 crypto library ... not foundchecking for zlib library ... found 若出现上述问题则安装openssl 1yum -y install openssl openssl-devel 安装openssl完成后再次执行命令 1./configure --prefix=/usr/local/nginx 出现下图信息则说明配置成功 4.安装 12makemake install 出现类似这样的就表示安装成功了 123456cp conf/nginx.conf '/usr/local/nginx/conf/nginx.conf.default'test -d '/usr/local/nginx/logs' || mkdir -p '/usr/local/nginx/logs'test -d '/usr/local/nginx/logs' || mkdir -p '/usr/local/nginx/logs'test -d '/usr/local/nginx/html' || cp -R html '/usr/local/nginx'test -d '/usr/local/nginx/logs' || mkdir -p '/usr/local/nginx/logs'make[1]: Leaving directory '/root/setup/nginx/nginx-1.14.1' 安装完后/usr/local/nginx 后出现几个文件夹conf、html、logs、sbin，配置nginx.conf在文件夹conf中 启动nginx 1./usr/nginx/sbin/nginx 部分nginx启动失败解决方法 关闭SELINUX 12vi /etc/selinux/config将SELINUX=enforcing改为SELINUX=disabled 这时候需要注意，开启nginx配置的防火墙端口 开启防火墙端口教程 安装redis1.安装redis 1yum install redis 2.下载fedora的epel仓库 1yum install epel-release 3.安装redis数据库 1yum install redis 4.修改redis.conf配置文件 123456789101112原文件：bind 127.0.0.1protected-mode yesappendonly no//持久化# requirepass foobared修改后：#bind 127.0.0.1protected-mode noappendonly yes//持久化requirepass yourpassword //redis密码 5.使用配置文件启动 redis 1redis-server /etc/redis.conf &amp; 6.启动redis相关命令 12345678# 启动redisservice redis start 或 systemctl start redis.service# 停止redisservice redis stop 或 systemctl stop redis.service# 查看redis运行状态service redis status 或 systemctl status redis.service# 查看redis进程ps -ef | grep redis 7.本机测试访问 12redis-cli -h 127.0.0.1 -p 6379quit 这时候需要注意，非本机访问redis需要开启防火墙端口 开启防火墙端口教程","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://midkuro.github.io/categories/Nginx/"},{"name":"Redis","slug":"Nginx/Redis","permalink":"https://midkuro.github.io/categories/Nginx/Redis/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://midkuro.github.io/tags/Linux/"},{"name":"Nginx","slug":"Nginx","permalink":"https://midkuro.github.io/tags/Nginx/"},{"name":"Redis","slug":"Redis","permalink":"https://midkuro.github.io/tags/Redis/"}]},{"title":"'centos7 设置应用程序为系统服务'","slug":"systemctl-use","date":"2020-05-20T12:30:00.000Z","updated":"2020-05-20T13:25:51.999Z","comments":true,"path":"2020/05/20/systemctl-use/","link":"","permalink":"https://midkuro.github.io/2020/05/20/systemctl-use/","excerpt":"","text":"centos7使用systemctl以下的 xxx为服务名称，需根据自身需求修改 1.进入目录 1cd /usr/lib/systemd/system/ 2.创建xxx.service文件 1vi xxx.service 3.赋予xxx.service文件权限 1chmod 754 xxx.service xxx.service文件详解[Unit] 部份 设置参数 参数意义说明 Description 服务名称 After 说明此服务 是在哪个服务启动之后才启动的意思！基本上仅是说明服务启动的顺序而已，并没有强制要求里头的服务一定要启动后此 unit 才能启动。 Before 与 After 的意义相反，是在什么服务启动前最好启动这个服务的意思。不过这仅是规范服务启动的顺序，并非强制要求的意思。 Requires 明确的定义此服务需要在哪个服务启动后才能够启动！就是设置相依服务！如果在此项设置的前导服务没有启动，那么此服务就不会被启动！ Conflicts 代表互斥的服务！亦即这个项目后面接的服务如果有启动，那么我们这个服务本身就不能启动！我们服务有启动，则此项目后的服务就不能启动！ [Service] 部份 设置参数 参数意义说明 Type 说明这个程序启动的方式，会影响到 ExecStart,一般来说，有下面几种类型 simple：默认值，这个程序主要由 ExecStart 接的指令串来启动，启动后常驻于内存中。forking：由 ExecStart 启动的程序通过 spawns 延伸出其他子程序来作为此程序 的主要服务。原生的父程序在启动结束后就会终止运行。 传统的 unit 服务大多属于这种项目.还有oneshot、dbus、idle等类型，请自行了解. EnvironmentFile 可以指定启动脚本的环境配置文件.例如 sshd.service 的配置文件写入到 /etc/sysconfig/sshd 当中！你也可以使用 Environment= 后面接多个不同的 Shell 变量来给予设置 ExecStart 启动应用程序的命令 ExecStop 停止应用程序的命令 ExecReload 重载应用程序的命令 Restart 当设置 Restart=1 时，则当此服务终止后，会再次的启动此服务 [Install] 部份 设置参数 参数意义说明 WantedBy 这个设置后面接的大部分是 *.target unit,意思是这个服务本身是附挂在哪一个target unit下面的,都是附挂在 multi-user.target下面 redis及nginx为例以redis为例，在该路径下 vi redis.service，并复制进去以下内容，进行相应修改. 123456789101112[unit]Description&#x3D;redis - high performance web serverAfter&#x3D;network.target remote-fs.target nss-lookup.target[Service]Type&#x3D;forkingExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;redis&#x2F;redis-5.0.2&#x2F;start.shExecReload&#x3D;ExecStop&#x3D;&#x2F;usr&#x2F;local&#x2F;redis&#x2F;redis-5.0.2&#x2F;stop.sh[Install]WantedBy&#x3D;multi-user.target 以nginx为例，在该路径下 vi nginx.service，并复制进去以下内容，进行相应修改. 123456789101112[unit]Description&#x3D;nginx - high performance web serverAfter&#x3D;network.target remote-fs.target nss-lookup.target[Service]Type&#x3D;forkingExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -c &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.confExecReload&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -s reloadExecStop&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -s stop[Install]WantedBy&#x3D;multi-user.target 比如我mtex-auth服务的启动需要依赖mtex-config服务，可以这样配置vi mtex-auth.service 123456789101112[Unit]Description&#x3D;mtex-auth - high performance web serverAfter&#x3D;network.target remote-fs.target nss-lookup.target mtex-configRequires&#x3D;mtex-config[Service]Type&#x3D;forkingExecStart&#x3D;&#x2F;home&#x2F;jenkins-running-shell&#x2F;mtex-auth&#x2F;start.shExecReload&#x3D;ExecStop&#x3D;&#x2F;home&#x2F;jenkins-running-shell&#x2F;mtex-auth&#x2F;stop.sh[Install]WantedBy&#x3D;multi-user.target 服务命令操作以nginx为例，保存nginx.service文件后赋予执行权限 1chmod 754 nginx.service nginx开机自启 1systemctl enable nginx.service 启动nginx 1systemctl start nginx.service 停止nginx 1systemctl stop nginx.service 重启nginx 1systemctl restart nginx.service centos7也可以使用旧版命令 system stop xxx 、system start xxx达到效果.","categories":[{"name":"Systemctl","slug":"Systemctl","permalink":"https://midkuro.github.io/categories/Systemctl/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://midkuro.github.io/tags/Linux/"},{"name":"Systemctl","slug":"Systemctl","permalink":"https://midkuro.github.io/tags/Systemctl/"}]},{"title":"'linux 开放防火墙端口'","slug":"firewalld-use","date":"2020-05-20T12:00:00.000Z","updated":"2020-05-20T13:22:08.301Z","comments":true,"path":"2020/05/20/firewalld-use/","link":"","permalink":"https://midkuro.github.io/2020/05/20/firewalld-use/","excerpt":"","text":"centos7开放防火墙端口启动防火墙 1systemctl start firewalld 停止防火墙 1systemctl stop firewalld 查看防火墙状态 1firewall-cmd --state 查看防火墙启动状态详情 1systemctl status firewalld 开机禁用 1systemctl disable firewalld 开机启用 1systemctl enable firewalld 查看所有打开的端口 1firewall-cmd --zone=public --list-ports 开启端口 1firewall-cmd --zone=public --add-port=80/tcp --permanent –permanent永久生效，没有此参数重启后失效 重新载入 1firewall-cmd --reload 查看端口状态 1firewall-cmd --zone=public --query-port=80/tcp 删除端口 1firewall-cmd --zone= public --remove-port=80/tcp --permanent","categories":[{"name":"Firewall","slug":"Firewall","permalink":"https://midkuro.github.io/categories/Firewall/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://midkuro.github.io/tags/Linux/"},{"name":"Firewall","slug":"Firewall","permalink":"https://midkuro.github.io/tags/Firewall/"}]},{"title":"'jenkins 参数化构建'","slug":"jenkins-param","date":"2020-05-20T07:50:40.000Z","updated":"2020-05-20T09:41:25.058Z","comments":true,"path":"2020/05/20/jenkins-param/","link":"","permalink":"https://midkuro.github.io/2020/05/20/jenkins-param/","excerpt":"","text":"jenkins参数化构建通过输入参数进行构建项目，需要插件build with parameters plugin，正常安装成功的jenkins应该都会自带了的，那么参数可以做什么事呢？如果我想要jenkins构建svn上指定版本号的代码并进行测试，就可以使用参数构建. 首先先了解一下怎么获取svn上指定的版本代码:通过项目路径@版本号进行获取的，版本号为head时，则取最新版本. 打开jenkins的任务配置，在General模块中找到参数化构建工程,勾选并添加自己的参数类型，本文选的是字符参数. 填写完相应的参数配置，需要修改项目svn的路径，将其改成项目路径@$参数名称 的格式.这时候的svn校验将失效，因为是配置了参数. 点击保存，就已经完成了参数化构建的配置了.这时候立即构建的按钮将变成Build with Parameters ,是不是觉得很简单？ 这时候又会发现，上下游均配置了参数化构建，但是触发了上游项目构建，并不能将参数传递到我的下游项目中，这时候会发现下游项目使用的是配置中的默认参数head,怎么做到传递一次参数即可触发构建呢？ 这时候要先安装一款插件Parameterized Trigger Plugin，安装完成后解除项目的上下游关系 然后编辑上游项目的任务配置，找到Post Steps模块,点击Add post-build step,能够看到多了一项选择，选中Trigger/call builds other projects 这里做的是正向配置上下游关系，上文解除了之前上下游关系，目的就是为了在这里通过参数传递进行配置。填入下游项目的工程名称，并点击Add Parameters添加参数，在这里我选择的是Predefined parameters,其他的暂时没有去了解.并填写传参的参数，格式为参数名=${参数名大写},这里我并有去测试小写能不能传递，亲们可以试试看… 这时候构建上游，也能够接收到同样的参数并指定的SVN版本号进行构建了,如果想要在shell命令中使用参数，也可以通过${参数名大写}进行取值. jenkins远程触发项目构建jenkins远程触发项目构建能够实现的功能有很多，本文主要讲解如何通过一个url进行触发构建. 打开任务配置并找到构建触发器模块，勾选触发远程构建并配置一个秘钥，这个秘钥相当于密码，密码错误的话不会触发构建. 如何调度已经说得很清楚了：http://IP:端口号/job/任务名称/build?token=秘钥 也可以使用参数进行远程触发：http://IP:端口号/job/任务名称/buildWithParameters?token=秘钥&amp;&amp;参数名=参数值 也就是说，这时候项目配置了参数构建及远程触发构建，就可以通过http请求的调度促使jenkins进行参数化远程构建… 这个时候会发现，如果我没有登录，它是不让我远程触发构建的，那我如何不需要登录就进行触发构建呢？ 首先先安装一个插件Build Authorization Token Root Plugin,然后登录用户-&gt;点击右上角的登录名-&gt;再点击设置 输入生成token的字符串，并生成一串token秘钥，切记拷贝生成的token秘钥，然后回到项目的构建触发器模块，将生成的token秘钥填入身份验证令牌中即可 这时候请求的url将发生变化: 无参请求:http://IP:端口号/buildByToken/build?job=任务名称&amp;token=秘钥 参数请求:http://IP:端口号/buildByToken/buildWithParameters?job=任务名称&amp;token=秘钥&amp;参数名=参数值","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://midkuro.github.io/categories/Jenkins/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://midkuro.github.io/tags/Jenkins/"}]},{"title":"'jenkins 上下游与持续集成构建'","slug":"jenkins-upstream","date":"2020-05-20T05:52:40.000Z","updated":"2020-05-20T09:41:10.369Z","comments":true,"path":"2020/05/20/jenkins-upstream/","link":"","permalink":"https://midkuro.github.io/2020/05/20/jenkins-upstream/","excerpt":"","text":"spring boot项目之间的引用触发上下游构建首先上下游关系是可以通过配置实现的，可以通过正向配置或反向配置实现，建议统一使用一种。 先理解一下上游下游的概念，目前我的dao项目作为一个基础类库，然后mtex-auth引入依赖dao，那么dao就是mtex-auth的上游，mtex-auth就是dao的下游。 在这情况下，上游dao主动去建立和下游的mtex-auth的关系，则为正向配置，而下游mtex-auth主动去建立和上游dao的关系，则为反向配置。 正向配置：在dao任务中找到 构建后操作模块增加构建后操作步骤，选择构建其他工程，在该路径填写mtex-auth任务名称,则dao构建成功后会主动触发mtex-auth任务进行构建 反向配置:在mtex任务中找到 构建触发器模块，勾选其他工程构建后触发并填写dao任务名称 在我的理解中，不管正向配置还是反向配置，作用都是一样的，都是建立上下游关系，上游构建成功后触发下游构建,所以建议只使用一种配置，没必要双向关系. 配置好上下游权限后，建议每一个下游任务的配置中增加限制，打开任务进入General模块，点击高级，勾选该项目上游正在构建时阻止该项目构建选项. 设想一下，如果存在项目关系如下： A项目作为B项目的上游 A项目作为C项目的上游 B项目作为C项目的上游 那么在A项目构建成功后，逻辑来讲它是会触发两个下游之间的构建，也就是B项目和C项目同时构建，等B项目构建完会再次触发一次C项目的构建。而通过勾选上游构建时阻止构建下游，就能避免这个问题。 最理想的构建顺序是 A-B-C ,那么在A任务的正向配置中，则不需要配置下游项目C，只需要配置下游项目B，反向配置也一样。 其次，在任务模块构建触发器中，建议关闭 Build whenever a SNAPSHOT dependency is built，因为该配置会根据pom文件的快照项目依赖自动创建上下游关系，导致和正反向配置重复并可能出现多次打包的情况。 jenkins持续集成构建任务关系依赖可能出现这种情况，A、B、C项目都作为D项目的上游，而A、B、C又是单独互不影响的项目，这时候它们是可以进行并发构建的，可是这样就会触发D项目的3次构建，这明显是不合理的，那么怎么做到 A、B、C项目同时构建，然后再触发下游D项目的构建呢？ 首先安装一个串行的插件 Multijob,它支持将任务捆绑构建。我们需要先解除A、B、C项目和D项目的上下游关系，也就是取消正反向配置,并且取消A、B、C任务中General模块的 该项目上游正在构建时阻止该项目构建配置，切记必须取消，否则无法进行构建. 新建任务，选择 Multijob Project,创建一个任务E。 找到构建模块，点击增加构建步骤并选择Multijob Phase,并依次添加A、B、C任务名称 这个时候可以点击每个任务右下角的高级进行详细配置 构建方式可以选择 串行或者并行，串行的话则按照添加任务的顺序进行构建，并行则同时构建。 构建条件可以选择 构建成功触发、构建失败触发、无论结果如何都触发等操作，按需求配置。 配置完成后关联E项目和D项目之间的上下游关系,并配置该项目上游正在构建时阻止该项目构建即可，就能满足触发E项目时， 触发A、B、C项目同时构建，然后再触发下游D项目的构建的操作。 这个时候如果项目是并行的，必须设置jenkins的最大并行执行器的数量,系统管理-&gt;系统设置-&gt;填写执行器数量。","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://midkuro.github.io/categories/Jenkins/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://midkuro.github.io/tags/Jenkins/"}]},{"title":"'jenkins 搭建简单的构建'","slug":"jenkins-use","date":"2020-05-20T05:51:40.000Z","updated":"2020-05-20T09:40:55.442Z","comments":true,"path":"2020/05/20/jenkins-use/","link":"","permalink":"https://midkuro.github.io/2020/05/20/jenkins-use/","excerpt":"","text":"jenkins插件安装及环境配置在使用jenkins之前，我们先学会配置jenkins的环境以及插件的安装，jenkins的环境配置均支持自动下载安装，但是不建议，也许是个人对环境路径存放位置具有强迫症吧。 首先进入jenkins页面，点击左侧 系统管理 ,然后找到 全局工具配置 然后选中 JDK及Maven的环境进行配置，点击新增将出现配置路径，将已安装的JDK及Maven路径配置上即可,对git有需要的可以自行配置. 配置完成后点击下方的save进行保存，接下来进入插件安装，点击插件管理，并进入可选插件界面 在该界面中，请不要使用右上角的 过滤 功能，由于插件过多，使用jenkins自带的过滤功能会导致浏览器卡死，所以请使用浏览器内容搜索的功能 ctrl + F在浏览器的搜索框中输入 Maven Integration 搜索maven插件，不同的插件版本命名可能略有差异，找到maven插件后在左边文本框中打钩，点击下方直接安装即可 在安装过程中，有一个安装后重启jenkins的设置，建议取消打钩，等待安装完成即可. jenkins基于maven编译简单的java项目在主界面中点击新建任务 进行创建，此时能够看到构建一个maven项目,该选项是需要安装 Maven Integration 才会出现的，选中它并输入项目名称，点击下方的确认按钮 上图的步骤2也可以通过输入一个已存在的任务名称，将任务的所有配置拷贝复制到新建任务当中. General：建议一定要勾选丢弃旧的构建，并配置构建保留天数及数量，可以配置10天、10个，感觉足以，不丢弃旧的构建容易把磁盘空间占满. 源码管理：选中SVN（Git操作也差不多）,并输入项目的svn路径，然后添加svn访问用户，输入账号密码即可，jenkins会自动帮你检测该账号能否访问svn路径并提示。 构建触发器配置： 第一个参数代表的是分钟 minute，取值 0~59； 第二个参数代表的是小时 hour，取值 0~23； 第三个参数代表的是天 day，取值 1~31； 第四个参数代表的是月 month，取值 1~12； 最后一个参数代表的是星期 week，取值 0~7，0 和 7 都是表示星期天。 常用例子: 每小时构建一次： * H/1 * * * 每隔5分钟构建一次： H/5 * * * * 每天8点30分构建一次： 30 8 * * * 每个小时的第10分钟构建一次： 10 * * * * 每周六日的1点10分构建一次： 10 1 * * 6,0 Pre Steps：构建前需要执行的一些操作，可以选择shell脚本、window命令等，这个根据需求去研究如何配置，暂时不细讲 Build： 建议使用clean install 替换 clean package 命令,clean package是把项目打包到target下，它并不会打包到maven的仓库，而clean install会打包进maven的仓库，可以避免一些不必要的问题。 比如我曾经遇见过的一个问题，A项目依赖了B项目，而B项目使用的是clean package命令，导致A项目打包的时候去maven仓库找不到B项目的jar包，所以A项目一直打包失败。 Post Steps：构建后需要执行的一些操作，同Pre Steps，其中构建不稳定指的是最近的5次构建中，曾经出现过构建失败。 构建的邮件发送通知以后再细讲，配置到这后一个简单的构建任务就已经完成了，点击保存，界面会出现新建的构建任务，点击右边的构建即可。 进入项目详情，左下角能够看到一些构建历史，点击构建历史能够查看每一次的构建详情，也能看到触发的构建原因，SVN更新的版本、信息等。","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://midkuro.github.io/categories/Jenkins/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://midkuro.github.io/tags/Jenkins/"}]},{"title":"'jenkins 安装及卸载'","slug":"jenkins-install","date":"2020-05-20T05:50:40.000Z","updated":"2020-05-20T09:41:55.078Z","comments":true,"path":"2020/05/20/jenkins-install/","link":"","permalink":"https://midkuro.github.io/2020/05/20/jenkins-install/","excerpt":"","text":"安装JDK使用jenkins，需要安装jdk及maven，可以自己安装于本机，也可以通过安装完jenkins后进行自动安装。 jdk-8u201-linux-x64.tar.gz 下载地址 官网地址 apache-maven-3.6.0-bin.tar.gz 下载地址 官网地址 如需更换版本请另行到官网中下载,在linux系统中安装包请选择 linux-x64.tar.gz后缀的安装包进行下载. 下文涉及到JDK及Maven版本相关的命令请自行修改成对应的版本. 在linux终端输入命令 cd /usr/local/ ，并创建java文件夹 mkdir java 执行 cd java 进入java路径中，并将下载的安装包上传至该路径 /usr/local/java下 然后执行命令解压下载的压缩包: tar -zxvf jdk-8u201-linux-x64.tar.gz 若提示错误则请先执行 yum install -y tar 安装压缩包命令再执行解压命令(仅限centos，其他系统请自行百度). 使用vi进入文件编辑模式，配置环境变量:vi /etc/profile 敲击键盘i进入编辑模式，在文件末尾添加以下内容: 12345export JAVA_HOME=/usr/local/java/jdk1.8.0_201export JAVA_BIN=/usr/local/java/jdk1.8.0_201/binexport PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$JAVA_HOME/bin:$PATH 敲击键盘:wq 表示退出vi编辑模式并保存修改 执行命令使系统环境变量配置重新加载：source /etc/profile 测试JDK安装是否成功,输入 javac 和java -version 是否安装成功.出现下图类似的信息即安装成功 安装Maven操作与JDK基本相同，Maven环境变量需要配置的内容： 12export MAVEN_HOME=/usr/local/maven/apache-maven-3.6.0export PATH=$PATH:$MAVEN_HOME/bin` 测试Maven安装是否成功，输入 mvn -v 或者 mvn -version 即可. 安装Jenkinsjenkins安装包 jenkins-2.138.3-1.1.noarch.rpm 下载地址 官网地址 不同版本的jenkins安装插件的成功率不一样，推荐使用该jenkins版本，支持中文，并且插件安装的成功率较高。 上传安装包至linux服务器任意路径下，并在该路径下执行 rpm -ivh jenkins-2.138.3-1.1.noarch.rpm 进行安装 安装成功后可查看jenkins默认安装目录： rpm -ql jenkins 可自定义修改jenkins配置文件： vi /etc/sysconfig/jenkins 123456#jenkins端口配置JENKINS_PORT=\"8080\"#启动jenkins的用户，最好使用root，否则会出现权限不够等问题JENKINS_USER=\"jenkins\"#jenkins的项目路径，建议将其改成 `/data/jenkins` 放在大空间的路径下，避免出现空间不足等问题JENKINS_HOME=\"/var/lib/jenkins\" 若修改了JENKINS_HOME 配置，则需执行 cp -r 原路径 目标路径 命令，其中 -r 参数表示若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件. 根据上文将/var/lib/jenkins/路径修改为 /data/jenkins,则执行的命令为 cp -r /var/lib/jenkins/* /data/jenkins/ 退出vi编辑模式并保存 然后设置jenkins服务开机自启动: systemctl enable jenkins.service 启动jenkins: systemtl start jenkins.service ，将start改为restart、stop分别为重启、停止jenkins 如果启动时报错 Starting Jenkins -bash: /usr/bin/java: No such file or directory，则需要编辑文件 vim /etc/init.d/jenkins，将/usr/bin/java改为自己java的地址，自己java地址的查看命令 which java 启动jenkins后，浏览器访问 http://ip:端口 第一次登录Jenkins会要求解锁，复制红色标记中的路径，执行命令 cat 红色标记的路径，将返回的密码填入浏览器页面中，点击continue继续 输入完成后会提示安装自定义插件还是推荐插件，此处我选择左边的推荐插件，安装过程可能由于网络原因导致失败，后续失败的可以在系统设置-插件管理里面卸载或者重新安装即可，也可以在插件安装完成后选择retry重新安装失败的插件，尝试多几次即可。 创建用户并登陆 看到以下界面则代表jenkins已安装成功，到这里linux下安装配置jenkins教程就结束了 卸载Jenkins依次执行以下命令彻底卸载Jenkins 123456service jenkins stopyum clean allyum -y remove jenkinsrm -rf /var/cache/jenkins#请修改为自身机器的jenkins的路径rm -rf /var/lib/jenkins/","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://midkuro.github.io/categories/Jenkins/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://midkuro.github.io/tags/Jenkins/"}]}],"categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://midkuro.github.io/categories/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://midkuro.github.io/categories/Docker/"},{"name":"Nodejs","slug":"Nodejs","permalink":"https://midkuro.github.io/categories/Nodejs/"},{"name":"Nginx","slug":"Nginx","permalink":"https://midkuro.github.io/categories/Nginx/"},{"name":"Redis","slug":"Nginx/Redis","permalink":"https://midkuro.github.io/categories/Nginx/Redis/"},{"name":"Systemctl","slug":"Systemctl","permalink":"https://midkuro.github.io/categories/Systemctl/"},{"name":"Firewall","slug":"Firewall","permalink":"https://midkuro.github.io/categories/Firewall/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://midkuro.github.io/categories/Jenkins/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://midkuro.github.io/tags/DevOps/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://midkuro.github.io/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://midkuro.github.io/tags/Docker/"},{"name":"Nodejs","slug":"Nodejs","permalink":"https://midkuro.github.io/tags/Nodejs/"},{"name":"Linux","slug":"Linux","permalink":"https://midkuro.github.io/tags/Linux/"},{"name":"Nginx","slug":"Nginx","permalink":"https://midkuro.github.io/tags/Nginx/"},{"name":"Redis","slug":"Redis","permalink":"https://midkuro.github.io/tags/Redis/"},{"name":"Systemctl","slug":"Systemctl","permalink":"https://midkuro.github.io/tags/Systemctl/"},{"name":"Firewall","slug":"Firewall","permalink":"https://midkuro.github.io/tags/Firewall/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://midkuro.github.io/tags/Jenkins/"}]}